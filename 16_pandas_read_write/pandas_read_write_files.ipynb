{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Dark Art of Coding:\n",
    "## Introduction to Python\n",
    "pandas: reading in files\n",
    "\n",
    "<img src='../universal_images/dark_art_logo.600px.png' width='300' style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing pandas, etc\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using pandas, it is common to import pandas as `pd` and to simply import the factory functions: `Series` and `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is adept at reading in **many, many** data formats \n",
    "\n",
    "To see which ones, you can type pd.read and use **tab completion**\n",
    "\n",
    "```python\n",
    "pd.read<Press the tab key>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from the clipboard\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading from the clipboard after we copy data from a table on a webpage.\n",
    "\n",
    "There is a sample file in the folder called:\n",
    "\n",
    "```bash\n",
    "sample_table.html\n",
    "```\n",
    "\n",
    "WARNING: if running this on a JupyterHub, this won't work as expected. IF you run this on a system where you have access to the system clipboard (i.e. run the notebook natively on your local laptop) then when open this file with your browser, you will see a table. When running locally, if you copy the table to your clipboard, pandas will be able to extract the table from the clipboard.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NON-EXECUTABLE CELL (for demo purposes only)\n",
    "\n",
    "webdata = pd.read_clipboard()\n",
    "webdata\n",
    "\n",
    "# NON-EXECUTABLE CELL (for demo purposes only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a DataFrame in memory. As with all DataFrames, you would be able to access:\n",
    "\n",
    "* the columns\n",
    "* the rows\n",
    "* the `.attributes`\n",
    "* the `.methods()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from CSV\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on `csv` and `sql` for remainder of this discussion.\n",
    "\n",
    "Let's dive into `csv` first.\n",
    "\n",
    "To read from `csv` files, we use the `.read_csv()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following results, notice the column headers.\n",
    "# By default, pandas will use the first\n",
    "#     row as a header row...  \n",
    "#     thus 'barry allen' shows up a the header for column 1.\n",
    "# Maybe NOT what you want...\n",
    "\n",
    "data = pd.read_csv('../universal_datasets/log_file.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we include a list of names in the function\n",
    "#     call, pandas will use those as the headers,\n",
    "#     instead of the first row.\n",
    "\n",
    "named_cols = pd.read_csv('../universal_datasets/log_file.csv',\n",
    "                         names=['name', \n",
    "                                'email', \n",
    "                                'fmip', \n",
    "                                'toip',\n",
    "                                'datetime', \n",
    "                                'lat', \n",
    "                                'long', \n",
    "                                'payload'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .info() method shows us some details \n",
    "#     about our new DataFrame\n",
    "#     * The number and names of the columns\n",
    "#     * The datatypes for each column\n",
    "#     * etc\n",
    "\n",
    "named_cols.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to see a single column, we can use \n",
    "#     bracket notation \n",
    "\n",
    "named_cols['fmip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will see it, but I don't recommend using the dot notation:\n",
    "#     df.column_name\n",
    "\n",
    "named_cols.fmip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to ingest all the lines from a file. \n",
    "\n",
    "Presuming that certain lines lack useful information... \n",
    "\n",
    "* metadata\n",
    "* header lines\n",
    "* document data, etc.\n",
    "\n",
    "In this case, let's skip rows 0, 1, and 2 from the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_rows = pd.read_csv('../universal_datasets/log_file_junk.csv', \n",
    "                           names=['name', 'email', 'fmip', 'toip',\n",
    "                                  'datetime', 'lat', 'long', 'payload'],\n",
    "                           skiprows=[0, 1, 2])\n",
    "\n",
    "skipped_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may receive files with alternate separators/delimiters. Pandas gives you tools to  \n",
    "deal with this situation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file uses a 'pipe' character as the separator.\n",
    "\n",
    "piped_data = pd.read_csv('../universal_datasets/log_file_pipes.csv',\n",
    "                         names=['name', 'email', 'fmip',\n",
    "                                'toip', 'datetime', 'lat',\n",
    "                                'long', 'payload'],\n",
    "                         sep='|')\n",
    "\n",
    "piped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was a short file, but when you have thousands\n",
    "#     of rows, sometimes you simply want a quick look at \n",
    "#     samples of the data.\n",
    "\n",
    "# .tail() and .head() are good examples of this.\n",
    "\n",
    "piped_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading in data, `pandas` assigns a default index of `0..n`. Sometimes we want to use something different than the default indexing.\n",
    "\n",
    "We **can choose** a particular column to be used as an index.\n",
    "\n",
    "Here we chose to use the `datetime` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "date_index = pd.read_csv('../universal_datasets/log_file.csv', \n",
    "                         names=['name', 'email', 'fmip', 'toip',\n",
    "                                'datetime', 'lat', 'long', 'payload'],\n",
    "                         index_col='datetime')\n",
    "\n",
    "date_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have an index, we can select data from the DataFrame\n",
    "# based on the index. In this case, since we just made the\n",
    "# date/time our index, we can\n",
    "# easily select rows based on the date/time stamps\n",
    "\n",
    "date_index.loc['2016-02-06T21:44:56':'2016-02-06T21:49:36']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Points!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your **text editor** create a simple script called:\n",
    "\n",
    "```bash\n",
    "my_read_01.py```\n",
    "\n",
    "Execute your script in the **IPython interpreter** using the command:\n",
    "\n",
    "```bash\n",
    "run my_read_01.py```\n",
    "\n",
    "1. With a text editor look inside the file `../universal_datasets/log_file_sign.csv` and identify the delimiter.\n",
    "1. Open the file using the pandas `.read_csv()` method and the following:\n",
    "   * Assign the following names to the columns, in this order:<br><br>\n",
    "```\n",
    "name\n",
    "email\n",
    "fmip\n",
    "toip\n",
    "datetime\n",
    "lat\n",
    "long\n",
    "payload_size\n",
    "```<br><br>\n",
    "   * Assign the correct delimiter based on what you found\n",
    "   * Skip the even numbered rows using `range()` to count up to 1000 stepping by twos\n",
    "   * Index the DataFrame using the `datetime` column\n",
    "1. Display the data between the index `2016-01-29T22:27:34` and `2016-01-28T22:34:28`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you complete this exercise, please put your green post-it on your monitor. \n",
    "\n",
    "If you want to continue on at your own-pace, please feel free to do so.\n",
    "\n",
    "<img src='../universal_images/green_sticky.300px.png' width='200' style='float:left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../universal_datasets/log_file_sign.csv',\n",
    "                 names=['name',\n",
    "                        'email',\n",
    "                        'fmip',\n",
    "                        'toip',\n",
    "                        'datetime',\n",
    "                        'lat',\n",
    "                        'long',\n",
    "                        'payload_size'],\n",
    "                sep='!',\n",
    "                index_col='datetime', \n",
    "                skiprows = range(0,1000,2))\n",
    "\n",
    "df.loc['2016-01-29T22:27:34':'2016-01-28T22:34:28']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files have missing data or markers indicating that data is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_na = pd.read_csv('../universal_datasets/log_file_na.csv', \n",
    "                      names=['name', 'email', 'fmip',\n",
    "                             'toip', 'datetime', 'lat',\n",
    "                             'long', 'payload'])\n",
    "\n",
    "data_na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can drop any rows that contain NaN data:\n",
    "\n",
    "data_na.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for NaN status and converting the particular values to an pandas NaN flag is a time consuming process that might not be optimal when loading data.\n",
    "\n",
    "You **can** turn this process off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_na = pd.read_csv('../universal_datasets/log_file_na.csv', \n",
    "                      names=['name', 'email', 'fmip',\n",
    "                             'toip', 'datetime', 'lat', \n",
    "                             'long', 'payload'],\n",
    "                      na_filter=False)\n",
    "\n",
    "data_na.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can provide a list of particular values to use as na values. \n",
    "\n",
    "Some files or software will use sentinels or flag values to represent a null value.\n",
    "\n",
    "NOTE: in this case, pandas will combine the na_values you give with the built-in na values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_na = pd.read_csv('../universal_datasets/log_file_na.csv', \n",
    "                      names=['name', 'email', 'fmip',\n",
    "                             'toip', 'datetime', 'lat',\n",
    "                             'long', 'payload'],\n",
    "                      na_values=['', '9999'])\n",
    "data_na.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_na = pd.read_csv('../universal_datasets/log_file_na.csv', \n",
    "                      names=['name', 'email', 'fmip',\n",
    "                             'toip', 'datetime', 'lat',\n",
    "                             'long', 'payload'],\n",
    "                         na_values=['', '9999'],\n",
    "                         keep_default_na=False)\n",
    "\n",
    "data_na.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It is possible to tell pandas how many rows to read using:\n",
    "# nrows\n",
    "\n",
    "\n",
    "data_na = pd.read_csv('../universal_datasets/log_file_na.csv', \n",
    "                      names=['name', 'email', 'fmip',\n",
    "                             'toip', 'datetime', 'lat',\n",
    "                             'long', 'payload'],\n",
    "                      na_values=['', '9999'], \n",
    "                      keep_default_na=False,\n",
    "                      nrows=7)\n",
    "data_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes the amount of data you need to process is \n",
    "#     too large to read into memory, so you need to process\n",
    "#     it portion by portion.\n",
    "# The chunksize argument allows you to identify how many\n",
    "#     rows to read in at a time\n",
    "\n",
    "data = pd.read_csv('../universal_datasets/log_file.csv', \n",
    "                   names=['name', 'email', 'fmip', \n",
    "                          'toip', 'datetime', 'lat',\n",
    "                          'long', 'payload'],\n",
    "                   chunksize=3)\n",
    "\n",
    "for chunk in data:\n",
    "    print('\\n### pre-processing')\n",
    "    print('### more pre-processing')\n",
    "    print('### even more pre-processing')\n",
    "    print(chunk[['name', 'lat', 'long', 'payload']])\n",
    "    print('subtotal sum:', chunk['payload'].sum())\n",
    "    print('### post processing\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want to convert data in one or more columns of your\n",
    "#     DataFrame, you can use functions to transform the data.\n",
    "\n",
    "# To convert multiple columns with different functions\n",
    "#     you can use dictionary to create a mapping that\n",
    "#     defines which conversion function(s)\n",
    "#     to use against which columns(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dsplitter(address):\n",
    "    userid, domain = address.split('@')\n",
    "    return domain\n",
    "\n",
    "def date_only(datetime):\n",
    "    return datetime.split('T')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../universal_datasets/log_file.csv', \n",
    "                   names=['name', 'email', 'fmip',\n",
    "                          'toip', 'datetime', 'lat',\n",
    "                          'long', 'payload'],\n",
    "                   converters={'email':dsplitter,\n",
    "                               'datetime':date_only})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you only want to retain certain columns, you can\n",
    "#     identify which columns to keep, using:\n",
    "# usecols\n",
    "\n",
    "\n",
    "data = pd.read_csv('../universal_datasets/log_file.csv', \n",
    "                   names=['name', 'email', 'fmip',\n",
    "                          'toip', 'datetime', 'lat',\n",
    "                          'long', 'payload'],\n",
    "                   usecols=['email', 'fmip', 'toip'])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas can read from sql databases easily...\n",
    "\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('../universal_datasets/log_file.sql')\n",
    "cur = conn.cursor()\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM customers\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_sql('''SELECT date, email, lat, long FROM customers\n",
    "                          WHERE name LIKE \"%wayne%\"''', conn)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_sql('''SELECT date, email, lat, long\n",
    "                     FROM customers\n",
    "                     WHERE name LIKE \"%wayne%\"''',\n",
    "                  conn,\n",
    "                  index_col='date')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to disk\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.to_csv('class_out.csv',\n",
    "           columns=['email', 'lat', 'long', 'name'],\n",
    "           header=True, sep='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Points!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your **text editor** create a simple script called:\n",
    "\n",
    "```bash\n",
    "my_read_02.py```\n",
    "\n",
    "Execute your script in the **IPython interpreter** using the command:\n",
    "\n",
    "```bash\n",
    "run my_read_02.py```\n",
    "\n",
    "1. Connect to the `customers` table in the sql database: `../universal_datasets/log_file.sql`\n",
    "1. Read from the connection using pandas `.read_sql()` method to create a DataFrame with these characteristics:\n",
    "    * Read in only the following columns: `email`, `lat`, `long`, and `date`\n",
    "    * Choose only the rows where the name contains the string `barry`\n",
    "1. Use DataFrame's `.head()` method to display just the first ten rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you complete this exercise, please put your green post-it on your monitor. \n",
    "\n",
    "If you want to continue on at your own-pace, please feel free to do so.\n",
    "\n",
    "<img src='../universal_images/green_sticky.300px.png' width='200' style='float:left'>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
